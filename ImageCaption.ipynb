{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're Vietnamese.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name_translate= \"Helsinki-NLP/opus-mt-vi-en\"  # English to Vietnamese\n",
    "tokenizer_translate = MarianTokenizer.from_pretrained(model_name_translate)\n",
    "model_translate = MarianMTModel.from_pretrained(model_name_translate)\n",
    "model_translate.to(\"cuda\")  # Move the model to the GPU\n",
    "\n",
    "def translate_text(text, src_lang=\"vi\", tgt_lang=\"en\"):\n",
    "    # Tokenize the input text\n",
    "    tokenized_text = tokenizer_translate(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        translation = model_translate.generate(**tokenized_text)\n",
    "    # Decode the generated tokens into text\n",
    "    translated_text = tokenizer_translate.decode(translation[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "# text = \"This is a test sentence for translation.\"\n",
    "text = \"chúng tôi là người Việt Nam\"\n",
    "translated_text = translate_text(text)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://media.newyorker.com/cartoons/63dc6847be24a6a76d90eb99/master/w_1160,c_limit/230213_a26611_838.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "# display(image.resize((596, 437)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIChallenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a98b9bdfb4407da4d0c3d8ccf2a486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Both `max_new_tokens` (=20) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two cartoon monsters sitting around a campfire\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[\n",
    "    0\n",
    "].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def scan_file_path_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            lst.append(os.path.join(root, file))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def scan_file_name_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            # remove the extension\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            lst.append(file_name)\n",
    "    return lst\n",
    "\n",
    "\n",
    "audio_text = r\"Data\\rawData\\Audio_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L01_V001', 'L01_V002', 'L01_V003', 'L01_V004', 'L01_V005', 'L01_V006', 'L01_V007', 'L01_V008', 'L01_V009', 'L01_V010', 'L01_V011', 'L01_V012', 'L01_V013', 'L01_V014', 'L01_V015', 'L01_V016', 'L01_V017', 'L01_V018', 'L01_V019', 'L01_V020', 'L01_V021', 'L01_V022', 'L01_V023', 'L01_V024', 'L01_V025', 'L01_V026', 'L01_V027', 'L01_V028', 'L01_V029', 'L01_V030', 'L01_V031', 'L02_V001', 'L02_V002', 'L02_V003', 'L02_V004', 'L02_V005', 'L02_V006', 'L02_V007', 'L02_V008', 'L02_V009', 'L02_V010', 'L02_V011', 'L02_V012', 'L02_V013', 'L02_V014', 'L02_V015', 'L02_V016', 'L02_V017', 'L02_V018', 'L02_V019', 'L02_V020', 'L02_V021', 'L02_V022', 'L02_V023', 'L02_V024', 'L02_V025', 'L02_V026', 'L02_V027', 'L02_V028', 'L02_V029', 'L02_V030', 'L02_V031', 'L03_V001', 'L03_V002', 'L03_V003', 'L03_V004', 'L03_V005', 'L03_V006', 'L03_V007', 'L03_V008', 'L03_V009', 'L03_V010', 'L03_V011', 'L03_V012', 'L03_V013', 'L03_V014', 'L03_V015', 'L03_V016', 'L03_V017', 'L03_V018', 'L03_V019', 'L03_V020', 'L03_V021', 'L03_V022', 'L03_V023', 'L03_V024', 'L03_V025', 'L03_V026', 'L03_V027', 'L03_V028', 'L03_V029', 'L03_V030', 'L04_V001', 'L04_V002', 'L04_V003', 'L04_V004', 'L04_V005', 'L04_V006', 'L04_V007', 'L04_V008', 'L04_V009', 'L04_V010', 'L04_V011', 'L04_V012', 'L04_V013', 'L04_V014', 'L04_V015', 'L04_V016', 'L04_V017', 'L04_V018', 'L04_V019', 'L04_V020', 'L04_V021', 'L04_V022', 'L04_V023', 'L04_V024', 'L04_V025', 'L04_V026', 'L04_V027', 'L04_V028', 'L04_V029', 'L04_V030', 'L05_V001', 'L05_V002', 'L05_V003', 'L05_V004', 'L05_V005', 'L05_V006', 'L05_V007', 'L05_V008', 'L05_V009', 'L05_V010', 'L05_V011', 'L05_V012', 'L05_V013', 'L05_V014', 'L05_V015', 'L05_V016', 'L05_V017', 'L05_V018', 'L05_V019', 'L05_V020', 'L05_V021', 'L05_V022', 'L05_V023', 'L05_V024', 'L05_V025', 'L05_V026', 'L05_V027', 'L05_V028', 'L05_V029', 'L05_V030', 'L05_V031', 'L06_V001', 'L06_V002', 'L06_V003', 'L06_V004', 'L06_V005', 'L06_V006', 'L06_V007', 'L06_V008', 'L06_V009', 'L06_V010', 'L06_V011', 'L06_V012', 'L06_V013', 'L06_V014', 'L06_V015', 'L06_V016', 'L06_V017', 'L06_V018', 'L06_V019', 'L06_V020', 'L06_V021', 'L06_V022', 'L06_V023', 'L06_V024', 'L06_V025', 'L06_V026', 'L06_V027', 'L06_V028', 'L06_V029', 'L06_V030', 'L06_V031', 'L07_V001', 'L07_V002', 'L07_V003', 'L07_V004', 'L07_V005', 'L07_V006', 'L07_V007', 'L07_V008', 'L07_V009', 'L07_V010', 'L07_V011', 'L07_V012', 'L07_V013', 'L07_V014', 'L07_V015', 'L07_V016', 'L07_V017', 'L07_V018', 'L07_V019', 'L07_V020', 'L07_V021', 'L07_V022', 'L07_V023', 'L07_V024', 'L07_V025', 'L07_V026', 'L07_V027', 'L07_V028', 'L07_V029', 'L07_V030', 'L07_V031', 'L08_V001', 'L08_V002', 'L08_V003', 'L08_V004', 'L08_V005', 'L08_V006', 'L08_V007', 'L08_V008', 'L08_V009', 'L08_V010', 'L08_V011', 'L08_V012', 'L08_V013', 'L08_V014', 'L08_V015', 'L08_V016', 'L08_V017', 'L08_V018', 'L08_V019', 'L08_V020', 'L08_V021', 'L08_V022', 'L08_V023', 'L08_V024', 'L08_V025', 'L08_V026', 'L08_V027', 'L08_V028', 'L08_V029', 'L08_V030', 'L09_V001', 'L09_V002', 'L09_V003', 'L09_V004', 'L09_V005', 'L09_V006', 'L09_V007', 'L09_V008', 'L09_V009', 'L09_V010', 'L09_V011', 'L09_V012', 'L09_V013', 'L09_V014', 'L09_V015', 'L09_V016', 'L09_V017', 'L09_V018', 'L09_V019', 'L09_V020', 'L09_V021', 'L09_V022', 'L09_V023', 'L09_V024', 'L09_V025', 'L09_V026', 'L09_V027', 'L09_V028', 'L09_V029', 'L10_V001', 'L10_V002', 'L10_V003', 'L10_V004', 'L10_V005', 'L10_V006', 'L10_V007', 'L10_V008', 'L10_V009', 'L10_V010', 'L10_V011', 'L10_V012', 'L10_V013', 'L10_V014', 'L10_V015', 'L10_V016', 'L10_V017', 'L10_V018', 'L10_V019', 'L10_V020', 'L10_V021', 'L10_V022', 'L10_V023', 'L10_V024', 'L10_V025', 'L10_V026', 'L10_V027', 'L10_V028', 'L10_V029', 'L11_V001', 'L11_V002', 'L11_V003', 'L11_V004', 'L11_V005', 'L11_V006', 'L11_V007', 'L11_V008', 'L11_V009', 'L11_V010', 'L11_V011', 'L11_V012', 'L11_V013', 'L11_V014', 'L11_V015', 'L11_V016', 'L11_V017', 'L11_V018', 'L11_V019', 'L11_V020', 'L11_V021', 'L11_V022', 'L11_V023', 'L11_V024', 'L11_V025', 'L11_V026', 'L11_V027', 'L11_V028', 'L11_V029', 'L11_V030', 'L12_V001', 'L12_V002', 'L12_V003', 'L12_V004', 'L12_V005', 'L12_V006', 'L12_V007', 'L12_V008', 'L12_V009', 'L12_V010', 'L12_V011', 'L12_V012', 'L12_V013', 'L12_V014', 'L12_V015', 'L12_V016', 'L12_V017', 'L12_V018', 'L12_V019', 'L12_V020', 'L12_V021', 'L12_V022', 'L12_V023', 'L12_V024', 'L12_V025', 'L12_V026', 'L12_V027', 'L12_V028', 'L12_V029', 'L12_V030']\n"
     ]
    }
   ],
   "source": [
    "path_lst = scan_file_path_in_folder(\"Data\\\\PreporcessData\\\\Audio_key_text\\\\L01_V001\")\n",
    "name_lst = scan_file_name_in_folder(audio_text)\n",
    "print(name_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "# Optionally, you can suppress all warnings from the transformers library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [00:01<00:00, 307.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def process_image(i):\n",
    "    name = name_lst[i]\n",
    "    path_lst = scan_file_path_in_folder(\n",
    "        f\"Data\\\\rawData\\\\Keyframes\\\\Keyframes_{name[0:3]}\\\\keyframes\\\\{name}\"\n",
    "    )\n",
    "    sub_name_lst = scan_file_name_in_folder(\n",
    "        f\"Data\\\\rawData\\\\Keyframes\\\\Keyframes_{name[0:3]}\\\\keyframes\\\\{name}\"\n",
    "    )\n",
    "\n",
    "    for path, sub_name in zip(path_lst, sub_name_lst):\n",
    "        out_path = (\n",
    "            f\"Data\\\\PreporcessData\\\\keyframes_ImageCaption\\\\{name}_{sub_name}.txt\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # open the image\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            # Preprocess the image\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "            # Generate text\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "                generated_text = processor.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=True\n",
    "                )[0].strip()\n",
    "                text = translate_text(\"i \" + generated_text)\n",
    "\n",
    "            # Translate text\n",
    "            # text = translator.translate(\"i \" + generated_text, src=\"en\", dest=\"vi\").text\n",
    "\n",
    "            # Write the text to the file\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "\n",
    "# Using ThreadPoolExecutor to process images in parallel\n",
    "with ThreadPoolExecutor(\n",
    "    max_workers=16\n",
    ") as executor:  # Adjust the number of workers based on your CPU\n",
    "    list(\n",
    "        tqdm(\n",
    "            executor.map(process_image, range(0, len(name_lst), 1)),\n",
    "            total=len(name_lst),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(0, len(name_lst), 2)):\n",
    "#     name = name_lst[i]\n",
    "#     path_lst = scan_file_path_in_folder(\n",
    "#         f\"Data\\\\rawData\\\\Keyframes\\\\Keyframes_{name[0:3]}\\\\keyframes\\\\{name}\"\n",
    "#     )\n",
    "#     sub_name_lst = scan_file_name_in_folder(\n",
    "#         f\"Data\\\\rawData\\\\Keyframes\\\\Keyframes_{name[0:3]}\\\\keyframes\\\\{name}\"\n",
    "#     )\n",
    "#     for path, sub_name in zip(path_lst, sub_name_lst):\n",
    "#         out_path = (\n",
    "#             f\"Data\\\\PreporcessData\\\\keyframes_ImageCaption\\\\{name}_{sub_name}.txt\"\n",
    "#         )\n",
    "#         # open the image\n",
    "#         image = Image.open(path).convert(\"RGB\")\n",
    "#         # check if the file is already exist\n",
    "#         if os.path.exists(out_path):\n",
    "#             continue\n",
    "\n",
    "#         inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "#         with torch.no_grad():\n",
    "#             generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "#         generated_text = processor.batch_decode(\n",
    "#             generated_ids, skip_special_tokens=True\n",
    "#         )[0].strip()\n",
    "#         text = translator.translate(\"i \" + generated_text, src=\"en\", dest=\"vi\").text\n",
    "#         # write the text to the file\n",
    "#         with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
