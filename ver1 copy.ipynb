{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PJF(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention_2): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (atte_reduce): AttentionReduce(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (ffn_2): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1536, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from ModelAi.projectFeature import PJFTrainModel, PJF\n",
    "import pandas as pd\n",
    "\n",
    "PJF_model = PJF()\n",
    "PJF_model.to(\"cuda\")\n",
    "model = PJFTrainModel(model=PJF_model)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"D:\\\\AIChallenge\\\\Checkpoints\\\\model_epoch_{2}.pt\",\n",
    "    )\n",
    ")\n",
    "\n",
    "PJF_model = model.model\n",
    "PJF_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIChallenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\AIChallenge\\venv\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BARTpho-syllable\n",
    "# syllable_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "# bartpho_syllable = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n",
    "# TXT = \"Chúng tôi là những nghiên cứu viên.\"\n",
    "# input_ids = syllable_tokenizer(TXT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# features = bartpho_syllable(input_ids)\n",
    "\n",
    "# BARTpho-word\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word-base\")\n",
    "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word-base\")\n",
    "# to cuda\n",
    "bartpho_word.to(\"cuda\")\n",
    "bartpho_word.eval()\n",
    "for param in bartpho_word.parameters():\n",
    "    param.requires_grad = False\n",
    "# TXT = \"Chúng_tôi là những nghiên_cứu_viên .\"\n",
    "# input_ids = word_tokenizer(TXT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# features = bartpho_word(input_ids.to(\"cuda\"))\n",
    "\n",
    "# input_ids = word_tokenizer.encode(TXT, return_tensors=\"pt\").to(\"cuda\")\n",
    "# print(input_ids)\n",
    "# # check if input_ids is in cuda\n",
    "# features = bartpho_word(input_ids).last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chúng_tôi là những nghiên_cứu_viên . \n"
     ]
    }
   ],
   "source": [
    "# Word segmentation\n",
    "import py_vncorenlp\n",
    "\n",
    "# py_vncorenlp.download_model(save_dir=r\"D:\\VQA\\Notebook\\VnCoreNLP\")\n",
    "\n",
    "# Load the word and sentence segmentation component\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(\n",
    "    annotators=[\"wseg\"],\n",
    "    save_dir=r\"D:\\AIChallenge\\VnCoreNLP\",\n",
    ")\n",
    "\n",
    "# change dir to C:\\Users\\Asus.LAPTOP-8EU9PHJL.000\\Desktop\\VQA\\Notebook\n",
    "import os\n",
    "\n",
    "os.chdir(r\"D:\\AIChallenge\")\n",
    "\n",
    "\n",
    "def Word_Segmentation(text):\n",
    "    output = rdrsegmenter.word_segment(text)\n",
    "    if len(output) > 0:\n",
    "        # join all words in the list\n",
    "        res = \"\"\n",
    "        for i in output:\n",
    "            res += i + \" \"\n",
    "        return res\n",
    "    else:\n",
    "        output = \"\"\n",
    "    return output\n",
    "\n",
    "\n",
    "print(Word_Segmentation(\"Chúng tôi là những nghiên cứu viên.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def scan_file_path_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            lst.append(os.path.join(root, file))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def scan_file_name_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            # remove the extension\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            lst.append(file_name)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "audio_path = \"Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\"\n",
    "audio_path_lst = scan_file_path_in_folder(audio_path)\n",
    "audio_name_lst = scan_file_name_in_folder(audio_path)\n",
    "audio_path_dict = dict(zip(audio_name_lst, audio_path_lst))\n",
    "# audio key\n",
    "audio_key_path = \"Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\"\n",
    "audio_key_path_lst = scan_file_path_in_folder(audio_key_path)\n",
    "audio_key_name_lst = scan_file_name_in_folder(audio_key_path)\n",
    "audio_key_path_dict = dict(zip(audio_key_name_lst, audio_key_path_lst))\n",
    "# key image caption\n",
    "key_image_caption_path = (\n",
    "    \"Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\"\n",
    ")\n",
    "key_image_caption_path_lst = scan_file_path_in_folder(key_image_caption_path)\n",
    "key_image_caption_name_lst = scan_file_name_in_folder(key_image_caption_path)\n",
    "key_image_caption_path_dict = dict(\n",
    "    zip(key_image_caption_name_lst, key_image_caption_path_lst)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 106589 106589\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_path_dict), len(audio_key_path_dict), len(key_image_caption_path_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L01_V001': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\\\\L01_V001.pt', 'L01_V002': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\\\\L01_V002.pt', 'L01_V003': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\\\\L01_V003.pt', 'L01_V004': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\\\\L01_V004.pt', 'L01_V005': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\\\\L01_V005.pt'}\n",
      "{'L01_V001_001': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\\\\L01_V001_001.pt', 'L01_V001_002': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\\\\L01_V001_002.pt', 'L01_V001_003': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\\\\L01_V001_003.pt', 'L01_V001_004': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\\\\L01_V001_004.pt', 'L01_V001_005': 'Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\\\\L01_V001_005.pt'}\n",
      "{'L01_V001_001': 'Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\\\\L01_V001_001.pt', 'L01_V001_002': 'Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\\\\L01_V001_002.pt', 'L01_V001_003': 'Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\\\\L01_V001_003.pt', 'L01_V001_004': 'Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\\\\L01_V001_004.pt', 'L01_V001_005': 'Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\\\\L01_V001_005.pt'}\n"
     ]
    }
   ],
   "source": [
    "# print first 5 key, value in the dictionary\n",
    "print({k: audio_path_dict[k] for k in list(audio_path_dict)[:5]})\n",
    "print({k: audio_key_path_dict[k] for k in list(audio_key_path_dict)[:5]})\n",
    "print(\n",
    "    {k: key_image_caption_path_dict[k] for k in list(key_image_caption_path_dict)[:5]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [00:00<00:00, 4776.29it/s]\n",
      "100%|██████████| 106589/106589 [00:11<00:00, 8935.87it/s] \n",
      "100%|██████████| 106589/106589 [00:08<00:00, 13264.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_paths(data_dict):\n",
    "    processed_dict = {}\n",
    "    for key, path in data_dict.items():\n",
    "        # to GPU\n",
    "        tensor = torch.load(path, map_location=\"cuda\")\n",
    "        processed_dict[key] = {\n",
    "            # \"path\": path,\n",
    "            \"tensor\": tensor,\n",
    "            # \"square_sum_tensor\": torch.sum(tensor**2),\n",
    "        }\n",
    "    return processed_dict\n",
    "\n",
    "\n",
    "def load_data_parallel(data_dict):\n",
    "    processed_dict = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_paths, {key: path}): key\n",
    "            for key, path in data_dict.items()\n",
    "        }\n",
    "        for future in tqdm(\n",
    "            concurrent.futures.as_completed(futures), total=len(data_dict)\n",
    "        ):\n",
    "            result = future.result()\n",
    "            processed_dict.update(result)\n",
    "    return processed_dict\n",
    "\n",
    "\n",
    "# Load all data in parallel into RAM\n",
    "video_dict = load_data_parallel(audio_path_dict)\n",
    "audio_key_dict = load_data_parallel(audio_key_path_dict)\n",
    "key_frame_dict = load_data_parallel(key_image_caption_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pack1-groupA\\\\query-p1-1-qa.txt', 'pack1-groupA\\\\query-p1-10-kis.txt', 'pack1-groupA\\\\query-p1-11-kis.txt', 'pack1-groupA\\\\query-p1-12-kis.txt', 'pack1-groupA\\\\query-p1-13-kis.txt', 'pack1-groupA\\\\query-p1-14-kis.txt', 'pack1-groupA\\\\query-p1-15-kis.txt', 'pack1-groupA\\\\query-p1-16-kis.txt', 'pack1-groupA\\\\query-p1-17-kis.txt', 'pack1-groupA\\\\query-p1-18-kis.txt', 'pack1-groupA\\\\query-p1-19-kis.txt', 'pack1-groupA\\\\query-p1-2-qa.txt', 'pack1-groupA\\\\query-p1-20-kis.txt', 'pack1-groupA\\\\query-p1-21-kis.txt', 'pack1-groupA\\\\query-p1-22-kis.txt', 'pack1-groupA\\\\query-p1-23-kis.txt', 'pack1-groupA\\\\query-p1-24-qa.txt', 'pack1-groupA\\\\query-p1-25-qa.txt', 'pack1-groupA\\\\query-p1-3-qa.txt', 'pack1-groupA\\\\query-p1-4-kis.txt', 'pack1-groupA\\\\query-p1-5-kis.txt', 'pack1-groupA\\\\query-p1-6-kis.txt', 'pack1-groupA\\\\query-p1-7-kis.txt', 'pack1-groupA\\\\query-p1-8-kis.txt', 'pack1-groupA\\\\query-p1-9-kis.txt']\n"
     ]
    }
   ],
   "source": [
    "query_path = \"pack1-groupA\"\n",
    "query_path_lst = scan_file_path_in_folder(query_path)\n",
    "query_path_name_lst = scan_file_name_in_folder(query_path)\n",
    "print(query_path_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensors = torch.stack([v[\"tensor\"] for v in audio_key_dict.values()])\n",
    "keyframe_tensors = torch.stack([v[\"tensor\"] for v in key_frame_dict.values()])\n",
    "video_tensors = torch.stack([v[\"tensor\"] for v in video_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m features \u001b[38;5;241m=\u001b[39m process_text(text)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Project features\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mPJF_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Calculate distances\u001b[39;00m\n",
      "File \u001b[1;32md:\\AIChallenge\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AIChallenge\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\AIChallenge\\ModelAi\\projectFeature.py:49\u001b[0m, in \u001b[0;36mPJF.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     48\u001b[0m     y, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_2(x, x, x)\n\u001b[1;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matte_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_2(x)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\AIChallenge\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\AIChallenge\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\AIChallenge\\ModelAi\\projectFeature.py:25\u001b[0m, in \u001b[0;36mAttentionReduce.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m att_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglimpses):\n\u001b[1;32m---> 25\u001b[0m     att_list\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mx_reduced\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     26\u001b[0m x_atted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(att_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_atted\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Cache to store already loaded CSV files\n",
    "map_keyframe_cache = {}\n",
    "\n",
    "\n",
    "# Tokenization and feature extraction\n",
    "def process_text(text):\n",
    "    text = Word_Segmentation(text)\n",
    "    # Remove punctuation and clean text\n",
    "    text = text.replace(\".\", \" \").replace(\",\", \" \").replace(\"!\", \" \").replace(\"?\", \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    lst_text = [\" \".join(words[i : i + 40]) for i in range(0, len(words), 40)]\n",
    "    lst_ids = word_tokenizer.batch_encode_plus(\n",
    "        lst_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=50,\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lst_outputs = bartpho_word(lst_ids)\n",
    "        lst_features = lst_outputs.last_hidden_state[\n",
    "            :, 0, :\n",
    "        ]  # Take the [CLS] token for each sequence\n",
    "\n",
    "    return lst_features\n",
    "\n",
    "\n",
    "# Distance calculation\n",
    "def calculate_distances(feature):\n",
    "    # Compute distances using vectorized operations\n",
    "    audio_distances = torch.norm(audio_tensors - feature, dim=1, p=2)\n",
    "    keyframe_distances = torch.norm(keyframe_tensors - feature, dim=1, p=2)\n",
    "    video_distances = torch.norm(video_tensors - feature, dim=1, p=2)\n",
    "\n",
    "    combined_distances = audio_distances + keyframe_distances\n",
    "    return combined_distances, video_distances\n",
    "\n",
    "\n",
    "# Main processing loop\n",
    "for idx in tqdm(range(len(query_path_lst))):\n",
    "    # Construct output path\n",
    "    tmp = query_path_name_lst[idx].split(\"-\")\n",
    "    output_path = f\"submit/{tmp[0]}-{tmp[2]}-{tmp[3]}.csv\"\n",
    "\n",
    "    # Load and process text\n",
    "    with open(query_path_lst[idx], \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    features = process_text(text)\n",
    "\n",
    "    # Project features\n",
    "    features = PJF_model(features)\n",
    "    feature = torch.sum(features, dim=0)\n",
    "\n",
    "    # Calculate distances\n",
    "    combined_distances, video_distances = calculate_distances(feature)\n",
    "\n",
    "    # Combine distances and match keys\n",
    "    distance = {}\n",
    "    for i, key in enumerate(key_image_caption_name_lst):\n",
    "        distance[key] = combined_distances[i]\n",
    "        for k_v, video_key in enumerate(audio_name_lst):\n",
    "            if video_key in key:\n",
    "                distance[key] += video_distances[k_v]\n",
    "                break\n",
    "\n",
    "    # Sort and get top 100 results\n",
    "    sorted_distance = sorted(distance.items(), key=lambda item: item[1])[:100]\n",
    "    top_100 = [key for key, _ in sorted_distance]\n",
    "\n",
    "    output_lines = []\n",
    "    for i in top_100:\n",
    "        name_video = i[:8]\n",
    "        key_frame = int(i[-3:])\n",
    "\n",
    "        # Check CSV cache\n",
    "        if name_video not in map_keyframe_cache:\n",
    "            df = pd.read_csv(f\"Data/rawData/map-keyframes/{name_video}.csv\")\n",
    "            map_keyframe_cache[name_video] = df\n",
    "        else:\n",
    "            df = map_keyframe_cache[name_video]\n",
    "\n",
    "        # Get real keyframe\n",
    "        real_key_frame = df[\"frame_idx\"].iloc[key_frame - 1]\n",
    "        output_lines.append(f\"{name_video},{real_key_frame}.\\n\")\n",
    "\n",
    "    # Write to output file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(output_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
