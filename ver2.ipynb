{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIChallenge\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\AIChallenge\\venv\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ModelAi.projectFeature import PJFTrainModel, PJF\n",
    "import pandas as pd\n",
    "\n",
    "PJF_model = PJF()\n",
    "PJF_model.to(\"cuda\")\n",
    "model = PJFTrainModel(model=PJF_model)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"D:\\\\AIChallenge\\\\Checkpoints\\\\model_epoch_{0}.pt\",\n",
    "    )\n",
    ")\n",
    "\n",
    "PJF_model = model.model\n",
    "PJF_model.eval()\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BARTpho-word\n",
    "word_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-word-base\")\n",
    "bartpho_word = AutoModel.from_pretrained(\"vinai/bartpho-word-base\")\n",
    "# to cuda\n",
    "bartpho_word.to(\"cuda\")\n",
    "bartpho_word.eval()\n",
    "for param in bartpho_word.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chúng_tôi là những nghiên_cứu_viên . \n"
     ]
    }
   ],
   "source": [
    "# Word segmentation\n",
    "import py_vncorenlp\n",
    "\n",
    "# py_vncorenlp.download_model(save_dir=r\"D:\\VQA\\Notebook\\VnCoreNLP\")\n",
    "\n",
    "# Load the word and sentence segmentation component\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(\n",
    "    annotators=[\"wseg\"],\n",
    "    save_dir=r\"D:\\AIChallenge\\VnCoreNLP\",\n",
    ")\n",
    "\n",
    "# change dir to C:\\Users\\Asus.LAPTOP-8EU9PHJL.000\\Desktop\\VQA\\Notebook\n",
    "import os\n",
    "\n",
    "os.chdir(r\"D:\\AIChallenge\")\n",
    "\n",
    "\n",
    "def Word_Segmentation(text):\n",
    "    output = rdrsegmenter.word_segment(text)\n",
    "    if len(output) > 0:\n",
    "        # join all words in the list\n",
    "        res = \"\"\n",
    "        for i in output:\n",
    "            res += i + \" \"\n",
    "        return res\n",
    "    else:\n",
    "        output = \"\"\n",
    "    return output\n",
    "\n",
    "\n",
    "print(Word_Segmentation(\"Chúng tôi là những nghiên cứu viên.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def scan_file_path_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            lst.append(os.path.join(root, file))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def scan_file_name_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            # remove the extension\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            lst.append(file_name)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file\n",
    "import json\n",
    "\n",
    "with open(\"clean_data.json\", \"r\") as f:\n",
    "    cleanData = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading video tensors: 100%|██████████| 726/726 [00:00<00:00, 2874.83it/s]\n",
      "Loading audio key tensors:  90%|████████▉ | 652/726 [01:02<00:07, 10.45it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     60\u001b[0m     concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(audio_key_tensors),\n\u001b[0;32m     61\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(audio_key_tensors),\n\u001b[0;32m     62\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading audio key tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     63\u001b[0m ):\n\u001b[0;32m     64\u001b[0m     video_name \u001b[38;5;241m=\u001b[39m audio_key_tensors[future]\n\u001b[1;32m---> 65\u001b[0m     cleanData[video_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_key_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     68\u001b[0m     concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(key_image_caption_tensors),\n\u001b[0;32m     69\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(key_image_caption_tensors),\n\u001b[0;32m     70\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading key image caption tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m ):\n\u001b[0;32m     72\u001b[0m     video_name \u001b[38;5;241m=\u001b[39m key_image_caption_tensors[future]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mprocess_list_tensor\u001b[1;34m(video_name, path_lst)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_list_tensor\u001b[39m(video_name, path_lst):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# stack all the tensors in the list\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath_lst\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_tensor(path):\n",
    "    # load tensor if it exists else return a tensor of zeros of shape (1, 1024)\n",
    "    res = torch.zeros(1, 1024)\n",
    "    if os.path.exists(path):\n",
    "        res = torch.load(path)\n",
    "    if res.shape[0] == 0:\n",
    "        res = torch.zeros(1, 1024)\n",
    "    return res\n",
    "\n",
    "\n",
    "def process_video(video_name, path):\n",
    "    return load_tensor(path)\n",
    "\n",
    "\n",
    "def process_list_tensor(video_name, path_lst):\n",
    "    # stack all the tensors in the list\n",
    "    return torch.stack([load_tensor(path) for path in path_lst])\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    tensors = {\n",
    "        executor.submit(process_video, video_name, val[\"path\"]): video_name\n",
    "        for video_name, val in cleanData.items()\n",
    "    }\n",
    "    audio_key_tensors = {\n",
    "        executor.submit(\n",
    "            process_list_tensor, video_name, val[\"audio_key_path_lst\"]\n",
    "        ): video_name\n",
    "        for video_name, val in cleanData.items()\n",
    "    }\n",
    "\n",
    "    key_image_caption_tensors = {\n",
    "        executor.submit(\n",
    "            process_list_tensor, video_name, val[\"key_image_caption_path_lst\"]\n",
    "        ): video_name\n",
    "        for video_name, val in cleanData.items()\n",
    "    }\n",
    "\n",
    "    BEITv2_tensors = {\n",
    "        executor.submit(\n",
    "            process_list_tensor, video_name, val[\"BEITv2_path_lst\"]\n",
    "        ): video_name\n",
    "        for video_name, val in cleanData.items()\n",
    "    }\n",
    "\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(tensors),\n",
    "        total=len(tensors),\n",
    "        desc=\"Loading video tensors\",\n",
    "    ):\n",
    "        video_name = tensors[future]\n",
    "        cleanData[video_name][\"video_tensor\"] = future.result()\n",
    "\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(audio_key_tensors),\n",
    "        total=len(audio_key_tensors),\n",
    "        desc=\"Loading audio key tensors\",\n",
    "    ):\n",
    "        video_name = audio_key_tensors[future]\n",
    "        cleanData[video_name][\"audio_key_tensor\"] = future.result()\n",
    "\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(key_image_caption_tensors),\n",
    "        total=len(key_image_caption_tensors),\n",
    "        desc=\"Loading key image caption tensors\",\n",
    "    ):\n",
    "        video_name = key_image_caption_tensors[future]\n",
    "        cleanData[video_name][\"key_image_caption_tensor\"] = future.result()\n",
    "\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(BEITv2_tensors),\n",
    "        total=len(BEITv2_tensors),\n",
    "        desc=\"Loading BEITv2 tensors\",\n",
    "    ):\n",
    "        video_name = BEITv2_tensors[future]\n",
    "        cleanData[video_name][\"BEITv2_tensor\"] = future.result()\n",
    "\n",
    "# stack all the video tensors of cleanData\n",
    "cleanData[\"video_tensors\"] = torch.stack(\n",
    "    [val[\"video_tensor\"] for val in cleanData.values()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_path = \"pack1-groupA\"\n",
    "query_path = \"pack2-groupA\"\n",
    "query_path_lst = scan_file_path_in_folder(query_path)\n",
    "query_path_name_lst = scan_file_name_in_folder(query_path)\n",
    "print(query_path_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(len(query_path)):\n",
    "    # read the text\n",
    "    with open(query_path_lst[i], \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
