{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PJF(\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention_2): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (atte_reduce): AttentionReduce(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (ffn_2): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from ModelAi.projectFeature import PJFTrainModel, PJF\n",
    "\n",
    "\n",
    "PJF_model = PJF()\n",
    "PJF_model.to(\"cuda\")\n",
    "model = PJFTrainModel(model=PJF_model)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"D:\\\\AIChallenge\\\\Checkpoints\\\\model_epoch_{2}.pt\",\n",
    "    )\n",
    ")\n",
    "\n",
    "PJF_model = model.model\n",
    "PJF_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def scan_file_path_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            lst.append(os.path.join(root, file))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def scan_file_name_in_folder(directory):\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # check files in the directory\n",
    "        for file in files:\n",
    "            # remove the extension\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            lst.append(file_name)\n",
    "    return lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "audio_path = \"Data\\\\PreporcessData\\\\Audio_text_bartpho\"\n",
    "audio_path_lst = scan_file_path_in_folder(audio_path)\n",
    "audio_name_lst = scan_file_name_in_folder(audio_path)\n",
    "audio_path_dict = dict(zip(audio_name_lst, audio_path_lst))\n",
    "# audio key\n",
    "audio_key_path = \"Data\\\\PreporcessData\\\\Audio_key_text_bartpho\"\n",
    "audio_key_path_lst = scan_file_path_in_folder(audio_key_path)\n",
    "audio_key_name_lst = scan_file_name_in_folder(audio_key_path)\n",
    "audio_key_path_dict = dict(zip(audio_key_name_lst, audio_key_path_lst))\n",
    "# key image caption\n",
    "key_image_caption_path = \"Data\\\\PreporcessData\\\\keyframes_ImageCaption_bartpho\"\n",
    "key_image_caption_path_lst = scan_file_path_in_folder(key_image_caption_path)\n",
    "key_image_caption_name_lst = scan_file_name_in_folder(key_image_caption_path)\n",
    "key_image_caption_path_dict = dict(\n",
    "    zip(key_image_caption_name_lst, key_image_caption_path_lst)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_path_dict 363\n",
      "['L01_V001', 'L01_V002', 'L01_V003', 'L01_V004', 'L01_V005', 'L01_V006', 'L01_V007', 'L01_V008', 'L01_V009', 'L01_V010', 'L01_V011', 'L01_V012', 'L01_V013', 'L01_V014', 'L01_V015', 'L01_V016', 'L01_V017', 'L01_V018', 'L01_V019', 'L01_V020', 'L01_V021', 'L01_V022', 'L01_V023', 'L01_V024', 'L01_V025', 'L01_V026', 'L01_V027', 'L01_V028', 'L01_V029', 'L01_V030', 'L01_V031', 'L02_V001', 'L02_V002', 'L02_V003', 'L02_V004', 'L02_V005', 'L02_V006', 'L02_V007', 'L02_V008', 'L02_V009', 'L02_V010', 'L02_V011', 'L02_V012', 'L02_V013', 'L02_V014', 'L02_V015', 'L02_V016', 'L02_V017', 'L02_V018', 'L02_V019', 'L02_V020', 'L02_V021', 'L02_V022', 'L02_V023', 'L02_V024', 'L02_V025', 'L02_V026', 'L02_V027', 'L02_V028', 'L02_V029', 'L02_V030', 'L02_V031', 'L03_V001', 'L03_V002', 'L03_V003', 'L03_V004', 'L03_V005', 'L03_V006', 'L03_V007', 'L03_V008', 'L03_V009', 'L03_V010', 'L03_V011', 'L03_V012', 'L03_V013', 'L03_V014', 'L03_V015', 'L03_V016', 'L03_V017', 'L03_V018', 'L03_V019', 'L03_V020', 'L03_V021', 'L03_V022', 'L03_V023', 'L03_V024', 'L03_V025', 'L03_V026', 'L03_V027', 'L03_V028', 'L03_V029', 'L03_V030', 'L04_V001', 'L04_V002', 'L04_V003', 'L04_V004', 'L04_V005', 'L04_V006', 'L04_V007', 'L04_V008', 'L04_V009', 'L04_V010', 'L04_V011', 'L04_V012', 'L04_V013', 'L04_V014', 'L04_V015', 'L04_V016', 'L04_V017', 'L04_V018', 'L04_V019', 'L04_V020', 'L04_V021', 'L04_V022', 'L04_V023', 'L04_V024', 'L04_V025', 'L04_V026', 'L04_V027', 'L04_V028', 'L04_V029', 'L04_V030', 'L05_V001', 'L05_V002', 'L05_V003', 'L05_V004', 'L05_V005', 'L05_V006', 'L05_V007', 'L05_V008', 'L05_V009', 'L05_V010', 'L05_V011', 'L05_V012', 'L05_V013', 'L05_V014', 'L05_V015', 'L05_V016', 'L05_V017', 'L05_V018', 'L05_V019', 'L05_V020', 'L05_V021', 'L05_V022', 'L05_V023', 'L05_V024', 'L05_V025', 'L05_V026', 'L05_V027', 'L05_V028', 'L05_V029', 'L05_V030', 'L05_V031', 'L06_V001', 'L06_V002', 'L06_V003', 'L06_V004', 'L06_V005', 'L06_V006', 'L06_V007', 'L06_V008', 'L06_V009', 'L06_V010', 'L06_V011', 'L06_V012', 'L06_V013', 'L06_V014', 'L06_V015', 'L06_V016', 'L06_V017', 'L06_V018', 'L06_V019', 'L06_V020', 'L06_V021', 'L06_V022', 'L06_V023', 'L06_V024', 'L06_V025', 'L06_V026', 'L06_V027', 'L06_V028', 'L06_V029', 'L06_V030', 'L06_V031', 'L07_V001', 'L07_V002', 'L07_V003', 'L07_V004', 'L07_V005', 'L07_V006', 'L07_V007', 'L07_V008', 'L07_V009', 'L07_V010', 'L07_V011', 'L07_V012', 'L07_V013', 'L07_V014', 'L07_V015', 'L07_V016', 'L07_V017', 'L07_V018', 'L07_V019', 'L07_V020', 'L07_V021', 'L07_V022', 'L07_V023', 'L07_V024', 'L07_V025', 'L07_V026', 'L07_V027', 'L07_V028', 'L07_V029', 'L07_V030', 'L07_V031', 'L08_V001', 'L08_V002', 'L08_V003', 'L08_V004', 'L08_V005', 'L08_V006', 'L08_V007', 'L08_V008', 'L08_V009', 'L08_V010', 'L08_V011', 'L08_V012', 'L08_V013', 'L08_V014', 'L08_V015', 'L08_V016', 'L08_V017', 'L08_V018', 'L08_V019', 'L08_V020', 'L08_V021', 'L08_V022', 'L08_V023', 'L08_V024', 'L08_V025', 'L08_V026', 'L08_V027', 'L08_V028', 'L08_V029', 'L08_V030', 'L09_V001', 'L09_V002', 'L09_V003', 'L09_V004', 'L09_V005', 'L09_V006', 'L09_V007', 'L09_V008', 'L09_V009', 'L09_V010', 'L09_V011', 'L09_V012', 'L09_V013', 'L09_V014', 'L09_V015', 'L09_V016', 'L09_V017', 'L09_V018', 'L09_V019', 'L09_V020', 'L09_V021', 'L09_V022', 'L09_V023', 'L09_V024', 'L09_V025', 'L09_V026', 'L09_V027', 'L09_V028', 'L09_V029', 'L10_V001', 'L10_V002', 'L10_V003', 'L10_V004', 'L10_V005', 'L10_V006', 'L10_V007', 'L10_V008', 'L10_V009', 'L10_V010', 'L10_V011', 'L10_V012', 'L10_V013', 'L10_V014', 'L10_V015', 'L10_V016', 'L10_V017', 'L10_V018', 'L10_V019', 'L10_V020', 'L10_V021', 'L10_V022', 'L10_V023', 'L10_V024', 'L10_V025', 'L10_V026', 'L10_V027', 'L10_V028', 'L10_V029', 'L11_V001', 'L11_V002', 'L11_V003', 'L11_V004', 'L11_V005', 'L11_V006', 'L11_V007', 'L11_V008', 'L11_V009', 'L11_V010', 'L11_V011', 'L11_V012', 'L11_V013', 'L11_V014', 'L11_V015', 'L11_V016', 'L11_V017', 'L11_V018', 'L11_V019', 'L11_V020', 'L11_V021', 'L11_V022', 'L11_V023', 'L11_V024', 'L11_V025', 'L11_V026', 'L11_V027', 'L11_V028', 'L11_V029', 'L11_V030', 'L12_V001', 'L12_V002', 'L12_V003', 'L12_V004', 'L12_V005', 'L12_V006', 'L12_V007', 'L12_V008', 'L12_V009', 'L12_V010', 'L12_V011', 'L12_V012', 'L12_V013', 'L12_V014', 'L12_V015', 'L12_V016', 'L12_V017', 'L12_V018', 'L12_V019', 'L12_V020', 'L12_V021', 'L12_V022', 'L12_V023', 'L12_V024', 'L12_V025', 'L12_V026', 'L12_V027', 'L12_V028', 'L12_V029', 'L12_V030']\n",
      "audio_key_path_dict 106589\n",
      "key_image_caption_path_dict 106589\n"
     ]
    }
   ],
   "source": [
    "print(\"audio_path_dict\", len(audio_path_dict))\n",
    "print(audio_name_lst)\n",
    "print(\"audio_key_path_dict\", len(audio_key_path_dict))\n",
    "print(\"key_image_caption_path_dict\", len(key_image_caption_path_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [00:04<00:00, 88.09it/s] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(audio_path_lst))):\n",
    "    input = torch.load(audio_path_lst[i])\n",
    "    output_path = \"Data\\\\PreporcessData\\\\Projection\\\\Audio_text_bartpho\\\\\" + audio_name_lst[i] + \".pt\"\n",
    "    output = PJF_model(input)\n",
    "    output = torch.sum(output, dim=0)\n",
    "    torch.save(output, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106589/106589 [11:23<00:00, 156.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(audio_key_path_lst))):\n",
    "    input = torch.load(audio_key_path_lst[i])\n",
    "    output_path = \"Data\\\\PreporcessData\\\\Projection\\\\Audio_key_text_bartpho\\\\\" + audio_key_name_lst[i] + \".pt\"\n",
    "    output = PJF_model(input)\n",
    "    output = torch.sum(output, dim=0)\n",
    "    torch.save(output, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106589/106589 [13:03<00:00, 136.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(key_image_caption_path_lst))):\n",
    "    input = torch.load(key_image_caption_path_lst[i])\n",
    "    output_path = (\n",
    "        \"Data\\\\PreporcessData\\\\Projection\\\\keyframes_ImageCaption_bartpho\\\\\"\n",
    "        + key_image_caption_name_lst[i]\n",
    "        + \".pt\"\n",
    "    )\n",
    "    output = PJF_model(input)\n",
    "    output = torch.sum(output, dim=0)\n",
    "    torch.save(output, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mim